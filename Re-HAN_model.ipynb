{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f41a4e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n2022.11.15，复现HAN(Heterogeneous Graph Attention Network)\\ndependencies:\\n    tensorflow-2.11.0\\n    numpy-1.22.0\\n    networkx-2.6.3\\n    scipy-1.7.3\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "2022.11.15，复现HAN(Heterogeneous Graph Attention Network)\n",
    "dependencies:\n",
    "    tensorflow-2.11.0\n",
    "    numpy-1.22.0\n",
    "    networkx-2.6.3\n",
    "    scipy-1.7.3\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e7c1403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "project_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a83d679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\PycharmProjects\\\\GNN_Event_Detection_models\\\\Re-HAN Models'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a87dee6",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f61b77",
   "metadata": {},
   "source": [
    "## data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dca23177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6e4615c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n Prepare adjacency matrix by expanding up to a given neighbourhood.\\n This will insert loops on every node.\\n Finally, the matrix is converted to bias vectors.\\n Expected shape: [graph, nodes, nodes]\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    " Prepare adjacency matrix by expanding up to a given neighbourhood.\n",
    " This will insert loops on every node.\n",
    " Finally, the matrix is converted to bias vectors.\n",
    " Expected shape: [graph, nodes, nodes]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c1903d",
   "metadata": {},
   "source": [
    "### adj_to_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19132eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 0., 1., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(3,4)[0] + np.eye(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a260b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_to_bias(adj, sizes, nhood=1):  # 邻接矩阵adjacency matrix\n",
    "    nb_graphs = adj.shape[0]  # 行,即num_nodes\n",
    "    mt = np.empty(adj.shape)  # 根据给定的维度和数值类型，返回一个新的ndarray数组，其元素不进行初始化\n",
    "    for g in range(nb_graphs):\n",
    "        mt[g] = np.eye(adj.shape[1])  # 返回一个二维的ndarray数组\n",
    "        for _ in range(nhood):\n",
    "            mt[g] = np.matmul(mt[g], (adj[g] + np.eye(adj.shape[1])))  # 相乘\n",
    "        for i in range(sizes[g]):  # 这个应该可以简化，直接对整个数组元素做操作！！！\n",
    "            for j in range(sizes[g]):\n",
    "                if mt[g][i][j] > 0.0:\n",
    "                    mt[g][i][j] = 1.0\n",
    "    return -1e9 * (1.0 - mt)  # 科学计数法，2.5 x 10^(-27)表示为：2.5e-27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485de35a",
   "metadata": {},
   "source": [
    "###  parse_index_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ce8a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5905152",
   "metadata": {},
   "source": [
    "### sample_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "573400ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成掩码bool数组\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)  # 生成全是0的数组\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f170368d",
   "metadata": {},
   "source": [
    "### sparse_to_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3658f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''转换为稀疏矩阵tuple'''\n",
    "def to_tuple(mx):\n",
    "    if not sp.isspmatrix_coo(mx):\n",
    "        mx = mx.tocoo()\n",
    "    coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "    values = mx.data\n",
    "    shape = mx.shape\n",
    "    return coords, values, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a14574b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e90dc9",
   "metadata": {},
   "source": [
    "### standardize_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d379e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(f, train_mask):\n",
    "    \"\"\"Standardize feature matrix and convert to tuple representation\"\"\"\n",
    "    # standardize data\n",
    "    f = f.todense()  # 将稀疏矩阵转回numpy矩阵\n",
    "    mu = f[train_mask == True, :].mean(axis=0)  # 按行求平均\n",
    "    sigma = f[train_mask == True, :].std(axis=0)\n",
    "    f = f[:, np.squeeze(np.array(sigma > 0))]  # sigma>0 get bool array\n",
    "    mu = f[train_mask == True, :].mean(axis=0)\n",
    "    sigma = f[train_mask == True, :].std(axis=0)\n",
    "    f = (f - mu) / sigma\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e8000",
   "metadata": {},
   "source": [
    "### preprocess_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1162540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()  # power数组元素求n次方，flatten是降到一维\n",
    "    r_inv[np.isinf(r_inv)] = 0.  # isinf判断是否为无穷\n",
    "    r_mat_inv = sp.diags(r_inv)  # 从对角线构造一个稀疏矩阵。\n",
    "    features = r_mat_inv.dot(features)  # dot矩阵乘法\n",
    "    return features.todense(), sparse_to_tuple(features)  # todense()转换成密集矩阵numpy.matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626d23fc",
   "metadata": {},
   "source": [
    "### normalize_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc2a875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix. 对称归一化邻接矩阵\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a996acf6",
   "metadata": {},
   "source": [
    "### preprocess_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16124e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))  # 对角线为1的矩阵\n",
    "    return sparse_to_tuple(adj_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1595f759",
   "metadata": {},
   "source": [
    "## layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "767be59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tf_slim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94785fad",
   "metadata": {},
   "source": [
    "### attn_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4824f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_head(features, out_sz, bias_mat, activation, in_drop=0.0, coef_drop=0.0, residual=False,\n",
    "              return_coef=False):\n",
    "    \"\"\"[summary]\n",
    "    multi-head attention计算\n",
    "    [description]\n",
    "    # forward；model = HeteGAT_multi\n",
    "    attns.append(layers.attn_head(features,            # list:3, tensor（1， 3025， 1870）\n",
    "                                bias_mat=bias_mat,     # list:2, tensor(1, 3025, 3025)\n",
    "                                out_sz=hid_units[0],   # hid_units:[8]，卷积核的个数\n",
    "                                activation=activation, # nonlinearity:tf.nn.elu\n",
    "                                in_drop=ffd_drop,      # tensor, ()\n",
    "                                coef_drop=attn_drop,   # tensor, ()\n",
    "                                residual=False))\n",
    "    Arguments:\n",
    "        features {[type]} -- shape=(batch_size, nb_nodes, fea_size))\n",
    "    \"\"\"\n",
    "    with tf.name_scope('my_attn'):  # 定义一个上下文管理器\n",
    "        if in_drop != 0.0:\n",
    "            features = tf.nn.dropout(features, 1.0 - in_drop)  # 以rate置0\n",
    "        features_fts = tf.compat.v1.layers.conv1d(features, out_sz, 1, use_bias=False)  # 一维卷积操作, out: (1, 3025, 8)\n",
    "        \n",
    "        f_1 = tf.compat.v1.layers.conv1d(features_fts, 1, 1)  # (1, 3025, 1)\n",
    "        f_2 = tf.compat.v1.layers.conv1d(features_fts, 1, 1)  # (1, 3025, 1)\n",
    "        \n",
    "        logits = f_1 + tf.transpose(f_2, [0, 2, 1])  # 转置         # (1, 3025, 3025)\n",
    "        coefs = tf.nn.softmax(tf.nn.leaky_relu(logits) + bias_mat)  # (1, 3025, 3025)\n",
    "\n",
    "        if coef_drop != 0.0:\n",
    "            coefs = tf.nn.dropout(coefs, 1.0 - coef_drop)           \n",
    "        if in_drop != 0.0:\n",
    "            features_fts = tf.nn.dropout(features_fts, 1.0 - in_drop)\n",
    "\n",
    "        vals = tf.matmul(coefs, features_fts)                       # (1, 3025, 8)\n",
    "        ret = tf_slim.bias_add(vals)  # 将bias向量加到value矩阵上      # (1. 3025， 8)\n",
    "\n",
    "        # residual connection 残差连接\n",
    "        if residual:\n",
    "            if features.shape[-1] != ret.shape[-1]:\n",
    "                ret = ret + tf.compat.v1.layers.conv1d(features, ret.shape[-1], 1)  # activation\n",
    "            else:\n",
    "                features_fts = ret + features\n",
    "        if return_coef:\n",
    "            return activation(ret), coefs\n",
    "        else:\n",
    "            return activation(ret)  # activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585b21e1",
   "metadata": {},
   "source": [
    "### attn_head_const_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22b7c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_head_const_1(seq, out_sz, bias_mat, activation, in_drop=0.0, coef_drop=0.0, residual=False):\n",
    "    \"\"\"[summary]\n",
    "    [description]\n",
    "    \"\"\"\n",
    "    adj_mat = 1.0 - bias_mat / -1e9\n",
    "    with tf.name_scope('my_attn'):\n",
    "        if in_drop != 0.0:\n",
    "            seq = tf.nn.dropout(seq, 1.0 - in_drop)\n",
    "        seq_fts = tf.compat.v1.layers.conv1d(seq, out_sz, 1, use_bias=False)\n",
    "        \n",
    "\n",
    "        logits = adj_mat \n",
    "        coefs = tf.nn.softmax(tf.nn.leaky_relu(logits) + bias_mat)\n",
    "\n",
    "        if coef_drop != 0.0:\n",
    "            coefs = tf.nn.dropout(coefs, 1.0 - coef_drop)\n",
    "        if in_drop != 0.0:\n",
    "            seq_fts = tf.nn.dropout(seq_fts, 1.0 - in_drop)\n",
    "\n",
    "        vals = tf.matmul(coefs, seq_fts)\n",
    "        ret = tf_slim.bias_add(vals)\n",
    "\n",
    "        # residual connection\n",
    "        if residual:\n",
    "            if seq.shape[-1] != ret.shape[-1]:\n",
    "                ret = ret + tf.compat.v1.layers.conv1d(seq, ret.shape[-1], 1)  # activation\n",
    "            else:\n",
    "                seq_fts = ret + seq\n",
    "\n",
    "        return activation(ret)  # activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a977a72",
   "metadata": {},
   "source": [
    "### sp_attn_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6ac3fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_attn_head(seq, out_sz, adj_mat, activation, nb_nodes, in_drop=0.0, coef_drop=0.0, residual=False):\n",
    "    with tf.name_scope('sp_attn'):\n",
    "        if in_drop != 0.0:\n",
    "            seq = tf.nn.dropout(seq, 1.0 - in_drop)\n",
    "\n",
    "        seq_fts = tf.compat.v1.layers.conv1d(seq, out_sz, 1, use_bias=False)\n",
    "\n",
    "        # simplest self-attention possible\n",
    "        f_1 = tf.compat.v1.layers.conv1d(seq_fts, 1, 1)\n",
    "        f_2 = tf.compat.v1.layers.conv1d(seq_fts, 1, 1)\n",
    "        logits = tf.sparse_add(adj_mat * f_1, adj_mat *\n",
    "                               tf.transpose(f_2, [0, 2, 1]))\n",
    "        lrelu = tf.SparseTensor(indices=logits.indices,\n",
    "                                values=tf.nn.leaky_relu(logits.values),\n",
    "                                dense_shape=logits.dense_shape)\n",
    "        coefs = tf.sparse_softmax(lrelu)  # 将softmax应用于批量的N维SparseTensor\n",
    "\n",
    "        if coef_drop != 0.0:\n",
    "            coefs = tf.SparseTensor(indices=coefs.indices,\n",
    "                                    values=tf.nn.dropout(\n",
    "                                        coefs.values, 1.0 - coef_drop),\n",
    "                                    dense_shape=coefs.dense_shape)\n",
    "        if in_drop != 0.0:\n",
    "            seq_fts = tf.nn.dropout(seq_fts, 1.0 - in_drop)\n",
    "\n",
    "        # As tf.sparse_tensor_dense_matmul expects its arguments to have rank-2,\n",
    "        # here we make an assumption that our input is of batch size 1, and reshape appropriately.\n",
    "        # The method will fail in all other cases!\n",
    "        coefs = tf.sparse_reshape(coefs, [nb_nodes, nb_nodes])\n",
    "        seq_fts = tf.squeeze(seq_fts)  \n",
    "        vals = tf.sparse_tensor_dense_matmul(coefs, seq_fts)  # SparseTensor稀疏矩阵乘法\n",
    "        vals = tf.expand_dims(vals, axis=0)  # 在0处扩展维度1\n",
    "        vals.set_shape([1, nb_nodes, out_sz])\n",
    "        ret = tf_slim.bias_add(vals)\n",
    "\n",
    "        # residual connection\n",
    "        if residual:\n",
    "            if seq.shape[-1] != ret.shape[-1]:\n",
    "                ret = ret + tf.compat.v1.layers.conv1d(seq, ret.shape[-1], 1)  # activation\n",
    "            else:\n",
    "                seq_fts = ret + seq\n",
    "\n",
    "        return activation(ret)  # activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3ae2a3",
   "metadata": {},
   "source": [
    "### SimpleAttLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ff4a53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_embed, att_val = layers.SimpleAttLayer(multi_embed, mp_att_size,\n",
    "#                                                      time_major=False,\n",
    "#                                                      return_alphas=True)\n",
    "def SimpleAttLayer(inputs, attention_size, time_major=False, return_alphas=False):\n",
    "    '''\n",
    "    inputs: tensor, (3025, 2, 64)\n",
    "    attention_size: 128\n",
    "    '''\n",
    "    if isinstance(inputs, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "        inputs = tf.concat(inputs, 2)  # 表示在shape第2个维度上拼接\n",
    "\n",
    "    if time_major:\n",
    "        # (T,B,D) => (B,T,D)\n",
    "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])  #\n",
    "\n",
    "    hidden_size = inputs.shape[2]  # D value - hidden size of the RNN layer\n",
    "\n",
    "    # Trainable parameters\n",
    "    w_omega = tf.Variable(tf.compat.v1.random_normal([hidden_size, attention_size], stddev=0.1))  # (64, 128)\n",
    "    b_omega = tf.Variable(tf.compat.v1.random_normal([attention_size], stddev=0.1))               # (128, )\n",
    "    u_omega = tf.Variable(tf.compat.v1.random_normal([attention_size], stddev=0.1))               # (128, )\n",
    "\n",
    "    with tf.name_scope('v'):\n",
    "        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)   # (3025, 2, 128)\n",
    "\n",
    "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
    "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape   tensor, (3025, 2)\n",
    "    alphas = tf.nn.softmax(vu, name='alphas')         # (B,T) shape   tensor, (3025, 2)\n",
    "\n",
    "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
    "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)  # (3025, 2, 64) * (3025, 2, 1) = (3025, 2, 64) -> (3025, 2)\n",
    "\n",
    "    if not return_alphas:\n",
    "        return output\n",
    "    else:\n",
    "        return output, alphas  # attention输出、softmax概率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72795e7",
   "metadata": {},
   "source": [
    "# clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15b06dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle  # 把训练好的模型存储起来\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score,adjusted_mutual_info_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import roc_curve, f1_score\n",
    "from sklearn import manifold  # 一种非线性降维的手段\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec7c941",
   "metadata": {},
   "source": [
    "## KNN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ab62687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_KNN(x, y, k=5, split_list=[0.2, 0.4, 0.6, 0.8], time=10, show_train=True, shuffle=True):\n",
    "    x = np.array(x)\n",
    "    x = np.squeeze(x)\n",
    "    y = np.array(y)\n",
    "    if len(y.shape) > 1:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    for split in split_list:\n",
    "        ss = split\n",
    "        split = int(x.shape[0] * split)\n",
    "        micro_list = []\n",
    "        macro_list = []\n",
    "        if time:\n",
    "            for i in range(time):\n",
    "                if shuffle:\n",
    "                    permutation = np.random.permutation(x.shape[0])  # 生成一个随机打散的序列。\n",
    "                    x = x[permutation, :]\n",
    "                    y = y[permutation]\n",
    "                # x_true = np.array(x_true)\n",
    "                train_x = x[:split, :]\n",
    "                test_x = x[split:, :]\n",
    "\n",
    "                train_y = y[:split]\n",
    "                test_y = y[split:]\n",
    "\n",
    "                estimator = KNeighborsClassifier(n_neighbors=k)\n",
    "                estimator.fit(train_x, train_y)\n",
    "                y_pred = estimator.predict(test_x)\n",
    "                f1_macro = f1_score(test_y, y_pred, average='macro')\n",
    "                f1_micro = f1_score(test_y, y_pred, average='micro')\n",
    "                macro_list.append(f1_macro)\n",
    "                micro_list.append(f1_micro)\n",
    "            print('KNN({}avg, split:{}, k={}) f1_macro: {:.4f}, f1_micro: {:.4f}'.format(\n",
    "                time, ss, k, sum(macro_list) / len(macro_list), sum(micro_list) / len(micro_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728cdedd",
   "metadata": {},
   "source": [
    "## kmeans_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5798d0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_Kmeans(x, y, k=4, time=10, return_NMI=False):\n",
    "\n",
    "    x = np.array(x)\n",
    "    x = np.squeeze(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    if len(y.shape) > 1:\n",
    "        y = np.argmax(y, axis=1)\n",
    "\n",
    "    estimator = KMeans(n_clusters=k)\n",
    "    ARI_list = []  # adjusted_rand_score(\n",
    "    NMI_list = []\n",
    "    AMI_list = []\n",
    "    if time:\n",
    "        # print('KMeans exps {}次 æ±~B平å~]~G '.format(time))\n",
    "        for i in range(time):\n",
    "            estimator.fit(x, y)\n",
    "            y_pred = estimator.predict(x)\n",
    "            score = normalized_mutual_info_score(y, y_pred)\n",
    "            NMI_list.append(score)\n",
    "            s2 = adjusted_rand_score(y, y_pred)\n",
    "            ARI_list.append(s2)\n",
    "            metric_ami = adjusted_mutual_info_score(y, y_pred)\n",
    "        # print('NMI_list: {}'.format(NMI_list))\n",
    "        nmi_score = np.mean(NMI_list)\n",
    "        ami_score = np.mean(AMI_list)\n",
    "        ari_score = np.mean(ARI_list)\n",
    "        print('NMI (10 avg): {:.4f} , AMI (10avg): {:.4f},  ARI (10avg): {:.4f}'.format(nmi_score, ami_score, ari_score))\n",
    "\n",
    "    else:\n",
    "        estimator.fit(x, y)\n",
    "        y_pred = estimator.predict(x)\n",
    "        score = normalized_mutual_info_score(y, y_pred)\n",
    "        print(\"NMI on all label data: {:.5f}\".format(score))\n",
    "    if return_NMI:\n",
    "        return nmi_score, ami_score, ari_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69896b23",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208d1fbd",
   "metadata": {},
   "source": [
    "## BaseGAttN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0fb822f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "var = tf.Variable(np.random.random(size=(1,)))\n",
    "print(type([var]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "873432fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ecedc5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseGAttN:\n",
    "    def loss(logits, labels, nb_classes, class_weights):\n",
    "        sample_wts = tf.reduce_sum(tf.multiply(\n",
    "            tf.one_hot(labels, nb_classes), class_weights), axis=-1)\n",
    "        xentropy = tf.multiply(tf.nn.sparse_softmax_cross_entropy_with_logits(  # 计算多分类交叉熵\n",
    "            labels=labels, logits=logits), sample_wts)\n",
    "        return tf.reduce_mean(xentropy, name='xentropy_mean')\n",
    "    \n",
    "    # 更新梯度权重\n",
    "    def training(loss, lr, l2_coef):\n",
    "        # weight decay\n",
    "        vars = tf.compat.v1.trainable_variables()  # 查看可训练变量,list\n",
    "        lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in vars if v.name not    # add_n实现列表相加；l2_loss是l2范数值得一半\n",
    "                           in ['bias', 'gamma', 'b', 'g', 'beta']]) * l2_coef\n",
    "        # lossL2, tensor(mul_5.0), shape=()\n",
    "        # optimizer\n",
    "        opt = tf.compat.v1.train.AdamOptimizer(learning_rate=lr)  # adam函数\n",
    "\n",
    "        # training op\n",
    "        train_op = opt.minimize(loss + lossL2)  # 计算梯度，然后更新参数\n",
    "\n",
    "        return train_op\n",
    "    \n",
    "    def preshape(logits, labels, nb_classes):\n",
    "        new_sh_lab = [-1]\n",
    "        new_sh_log = [-1, nb_classes]\n",
    "        log_resh = tf.reshape(logits, new_sh_log)\n",
    "        lab_resh = tf.reshape(labels, new_sh_lab)\n",
    "        return log_resh, lab_resh\n",
    "\n",
    "    def confmat(logits, labels):\n",
    "        preds = tf.argmax(logits, axis=1)  # 返回行最大值索引\n",
    "        return tf.confusion_matrix(labels, preds)  # 混淆矩阵\n",
    "    \n",
    "    \n",
    "    ##########################\n",
    "    # Adapted from tkipf/gcn #\n",
    "    ##########################\n",
    "\n",
    "    def masked_softmax_cross_entropy(logits, labels, mask):\n",
    "        \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(  # 返回交叉熵向量\n",
    "            logits=logits, labels=labels)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)  # 改变tensor数据类型\n",
    "        mask /= tf.reduce_mean(mask)  # 通过均值求loss\n",
    "        loss *= mask\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    def masked_sigmoid_cross_entropy(logits, labels, mask):\n",
    "        \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
    "        labels = tf.cast(labels, dtype=tf.float32)\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(  # sigmoid交叉熵\n",
    "            logits=logits, labels=labels)\n",
    "        loss = tf.reduce_mean(loss, axis=1)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        mask /= tf.reduce_mean(mask)\n",
    "        loss *= mask\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    def masked_accuracy(logits, labels, mask):\n",
    "        \"\"\"Accuracy with masking.\"\"\"\n",
    "        correct_prediction = tf.equal(   # 判断向量元素是否相等\n",
    "            tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "        accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        mask /= tf.reduce_mean(mask)\n",
    "        accuracy_all *= mask\n",
    "        return tf.reduce_mean(accuracy_all)\n",
    "    \n",
    "    def micro_f1(logits, labels, mask):\n",
    "        \"\"\"Accuracy with masking.\"\"\"\n",
    "        predicted = tf.round(tf.nn.sigmoid(logits))  # 四舍五入函数\n",
    "\n",
    "        # Use integers to avoid any nasty FP behaviour\n",
    "        predicted = tf.cast(predicted, dtype=tf.int32)\n",
    "        labels = tf.cast(labels, dtype=tf.int32)\n",
    "        mask = tf.cast(mask, dtype=tf.int32)\n",
    "\n",
    "        # expand the mask so that broadcasting works ([nb_nodes, 1])\n",
    "        mask = tf.expand_dims(mask, -1)\n",
    "\n",
    "        # Count true positives, true negatives, false positives and false negatives.\n",
    "        tp = tf.count_nonzero(predicted * labels * mask)  # 非零元素个数\n",
    "        tn = tf.count_nonzero((predicted - 1) * (labels - 1) * mask)\n",
    "        fp = tf.count_nonzero(predicted * (labels - 1) * mask)\n",
    "        fn = tf.count_nonzero((predicted - 1) * labels * mask)\n",
    "\n",
    "        # Calculate accuracy, precision, recall and F1 score.\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        fmeasure = (2 * precision * recall) / (precision + recall)\n",
    "        fmeasure = tf.cast(fmeasure, tf.float32)\n",
    "        return fmeasure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca9512b",
   "metadata": {},
   "source": [
    "## GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b5afdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4f6d2f",
   "metadata": {},
   "source": [
    "### GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0cd1a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(BaseGAttN):\n",
    "    def inference(inputs, nb_classes, nb_nodes, training, attn_drop, ffd_drop,\n",
    "                  bias_mat, hid_units, n_heads, activation=tf.nn.elu, residual=False):  # 残差\n",
    "        attns = []\n",
    "        for _ in range(n_heads[0]):\n",
    "            attns.append(attn_head(inputs, bias_mat=bias_mat,\n",
    "                                          out_sz=hid_units[0], activation=activation,\n",
    "                                          in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        h_1 = tf.concat(attns, axis=-1)\n",
    "        # multi-head attention\n",
    "        for i in range(1, len(hid_units)):\n",
    "            h_old = h_1\n",
    "            attns = []\n",
    "            for _ in range(n_heads[i]):\n",
    "                attns.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "                                              out_sz=hid_units[i], activation=activation,\n",
    "                                              in_drop=ffd_drop, coef_drop=attn_drop, residual=residual))\n",
    "            h_1 = tf.concat(attns, axis=-1)\n",
    "        out = []\n",
    "        for i in range(n_heads[-1]):\n",
    "            out.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "                                        out_sz=nb_classes, activation=lambda x: x,\n",
    "                                        in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        logits = tf.add_n(out) / n_heads[-1]\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a3622",
   "metadata": {},
   "source": [
    "### HeteGAT_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64137920",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteGAT_multi(BaseGAttN):\n",
    "    '''\n",
    "    # forward；model = HeteGAT_multi\n",
    "    logits, final_embedding, att_val = model.inference(ftr_in_list,  # list:3, tensor（1， 3025， 1870）\n",
    "                                                       nb_classes,   # 3\n",
    "                                                       nb_nodes,     # 3025\n",
    "                                                       is_train,     # bool\n",
    "                                                       attn_drop,    # tensor, ()\n",
    "                                                       ffd_drop,     # tensor, ()\n",
    "                                                        bias_mat_list=bias_in_list,  # list:2, tensor()\n",
    "                                                       hid_units=hid_units,   # hid_units:8\n",
    "                                                       n_heads=n_heads,       # n_heads: [8, 1]\n",
    "                                                       residual=residual,     # residual: False\n",
    "                                                       activation=nonlinearity)  # nonlinearity:tf.nn.elu\n",
    "\n",
    "    '''\n",
    "    def inference(ftr_in_list, nb_classes, nb_nodes, training, attn_drop, ffd_drop,\n",
    "                  bias_mat_list, hid_units, n_heads, activation=tf.nn.elu, residual=False,\n",
    "                  mp_att_size=128):\n",
    "        embed_list = []\n",
    "        for features, bias_mat in zip(ftr_in_list, bias_mat_list):\n",
    "            attns = []\n",
    "            jhy_embeds = []\n",
    "            for _ in range(n_heads[0]):   # [8,1]\n",
    "                # multi-head attention 计算\n",
    "                attns.append(attn_head(features, bias_mat=bias_mat,\n",
    "                                              out_sz=hid_units[0], activation=activation,\n",
    "                                              in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "            h_1 = tf.concat(attns, axis=-1)\n",
    "\n",
    "            for i in range(1, len(hid_units)):\n",
    "                h_old = h_1\n",
    "                attns = []\n",
    "                for _ in range(n_heads[i]):\n",
    "                    attns.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "                                                  out_sz=hid_units[i],\n",
    "                                                  activation=activation,\n",
    "                                                  in_drop=ffd_drop,\n",
    "                                                  coef_drop=attn_drop, residual=residual))\n",
    "                h_1 = tf.concat(attns, axis=-1)\n",
    "            embed_list.append(tf.expand_dims(tf.squeeze(h_1), axis=1))  # list:2. 其中每个元素tensor, (3025, 1, 64)\n",
    "\n",
    "        multi_embed = tf.concat(embed_list, axis=1)   # tensor, (3025, 2, 64)\n",
    "        # attention输出：tensor(3025, 64)、softmax概率\n",
    "        final_embed, att_val = SimpleAttLayer(multi_embed, \n",
    "                                              mp_att_size,\n",
    "                                              time_major=False,\n",
    "                                              return_alphas=True)\n",
    "\n",
    "        out = []\n",
    "        for i in range(n_heads[-1]):  # 1\n",
    "            # 用于添加一个全连接层(input, output) -> (3025, 3)\n",
    "            out.append(tf.compat.v1.layers.dense(final_embed, nb_classes, activation=None))  \n",
    "        #     out.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "        #                                 out_sz=nb_classes, activation=lambda x: x,\n",
    "        #                                 in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        logits = tf.add_n(out) / n_heads[-1]  # add_n是列表相加。tensor,(3025, 3)\n",
    "        # logits_list.append(logits)\n",
    "        print('de')\n",
    "\n",
    "        logits = tf.expand_dims(logits, axis=0)  # (1, 3025, 3)\n",
    "        # attention通过全连接层预测(1, 3025, 3)、attention final_embedding tensor(3025, 64)、attention 概率\n",
    "        return logits, final_embed, att_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7de40d",
   "metadata": {},
   "source": [
    "### HeteGAT_no_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f795a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteGAT_no_coef(BaseGAttN):\n",
    "    def inference(ftr_in_list, nb_classes, nb_nodes, is_train, attn_drop, ffd_drop,\n",
    "                  bias_mat_list, hid_units, n_heads, activation=tf.nn.elu, residual=False,\n",
    "                  mp_att_size=128):\n",
    "        embed_list = []\n",
    "        # coef_list = []\n",
    "        for bias_mat in bias_mat_list:\n",
    "            attns = []\n",
    "            head_coef_list = []\n",
    "            for _ in range(n_heads[0]):\n",
    "\n",
    "                attns.append(attn_head(ftr_in_list, bias_mat=bias_mat,\n",
    "                                                  out_sz=hid_units[0], activation=activation,\n",
    "                                                  in_drop=ffd_drop, coef_drop=attn_drop, residual=False,\n",
    "                                                  return_coef=return_coef))\n",
    "            h_1 = tf.concat(attns, axis=-1)\n",
    "            for i in range(1, len(hid_units)):\n",
    "                h_old = h_1\n",
    "                attns = []\n",
    "                for _ in range(n_heads[i]):\n",
    "                    attns.append(attn_head(h_1,\n",
    "                                                  bias_mat=bias_mat,\n",
    "                                                  out_sz=hid_units[i],\n",
    "                                                  activation=activation,\n",
    "                                                  in_drop=ffd_drop,\n",
    "                                                  coef_drop=attn_drop,\n",
    "                                                  residual=residual))\n",
    "                h_1 = tf.concat(attns, axis=-1)\n",
    "            embed_list.append(tf.expand_dims(tf.squeeze(h_1), axis=1))\n",
    "        # att for metapath\n",
    "        # prepare shape for SimpleAttLayer\n",
    "        # print('att for mp')\n",
    "        multi_embed = tf.concat(embed_list, axis=1)\n",
    "        final_embed, att_val = SimpleAttLayer(multi_embed, mp_att_size,\n",
    "                                                     time_major=False,\n",
    "                                                     return_alphas=True)\n",
    "        # print(att_val)\n",
    "        # last layer for clf\n",
    "        out = []\n",
    "        for i in range(n_heads[-1]):\n",
    "            out.append(tf.compat.v1.layers.dense(final_embed, nb_classes, activation=None))\n",
    "        #     out.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "        #                                 out_sz=nb_classes, activation=lambda x: x,\n",
    "        #                                 in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        logits = tf.add_n(out) / n_heads[-1]\n",
    "        # logits_list.append(logits)\n",
    "        print('de')\n",
    "        logits = tf.expand_dims(logits, axis=0)\n",
    "        # if return_coef:\n",
    "        #     return logits, final_embed, att_val, coef_list\n",
    "        # else:\n",
    "        return logits, final_embed, att_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145c6f62",
   "metadata": {},
   "source": [
    "### HeteGAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4586445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteGAT(BaseGAttN):\n",
    "    def inference(inputs, nb_classes, nb_nodes, training, attn_drop, ffd_drop,\n",
    "                  bias_mat_list, hid_units, n_heads, activation=tf.nn.elu, residual=False,\n",
    "                  mp_att_size=128,\n",
    "                  return_coef=False):\n",
    "        embed_list = []\n",
    "        coef_list = []\n",
    "        for bias_mat in bias_mat_list:\n",
    "            attns = []\n",
    "            head_coef_list = []\n",
    "            for _ in range(n_heads[0]):\n",
    "                if return_coef:\n",
    "                    a1, a2 = attn_head(inputs, bias_mat=bias_mat,\n",
    "                                              out_sz=hid_units[0], activation=activation,\n",
    "                                              in_drop=ffd_drop, coef_drop=attn_drop, residual=False,\n",
    "                                              return_coef=return_coef)\n",
    "                    attns.append(a1)\n",
    "                    head_coef_list.append(a2)\n",
    "                    # attns.append(attn_head(inputs, bias_mat=bias_mat,\n",
    "                    #                               out_sz=hid_units[0], activation=activation,\n",
    "                    #                               in_drop=ffd_drop, coef_drop=attn_drop, residual=False,\n",
    "                    #                               return_coef=return_coef)[0])\n",
    "                    #\n",
    "                    # head_coef_list.append(attn_head(inputs, bias_mat=bias_mat,\n",
    "                    #                                        out_sz=hid_units[0], activation=activation,\n",
    "                    #                                        in_drop=ffd_drop, coef_drop=attn_drop,\n",
    "                    #                                        residual=False,\n",
    "                    #                                        return_coef=return_coef)[1])\n",
    "                else:\n",
    "                    attns.append(attn_head(inputs, bias_mat=bias_mat,\n",
    "                                                  out_sz=hid_units[0], activation=activation,\n",
    "                                                  in_drop=ffd_drop, coef_drop=attn_drop, residual=False,\n",
    "                                                  return_coef=return_coef))\n",
    "            head_coef = tf.concat(head_coef_list, axis=0)\n",
    "            head_coef = tf.reduce_mean(head_coef, axis=0)\n",
    "            coef_list.append(head_coef)\n",
    "            h_1 = tf.concat(attns, axis=-1)\n",
    "            for i in range(1, len(hid_units)):\n",
    "                h_old = h_1\n",
    "                attns = []\n",
    "                for _ in range(n_heads[i]):\n",
    "                    attns.append(attn_head(h_1,\n",
    "                                                  bias_mat=bias_mat,\n",
    "                                                  out_sz=hid_units[i],\n",
    "                                                  activation=activation,\n",
    "                                                  in_drop=ffd_drop,\n",
    "                                                  coef_drop=attn_drop,\n",
    "                                                  residual=residual))\n",
    "                h_1 = tf.concat(attns, axis=-1)\n",
    "            embed_list.append(tf.expand_dims(tf.squeeze(h_1), axis=1))\n",
    "        # att for metapath\n",
    "        # prepare shape for SimpleAttLayer\n",
    "        # print('att for mp')\n",
    "        multi_embed = tf.concat(embed_list, axis=1)\n",
    "        final_embed, att_val = SimpleAttLayer(multi_embed, mp_att_size,\n",
    "                                                     time_major=False,\n",
    "                                                     return_alphas=True)\n",
    "        # print(att_val)\n",
    "        # last layer for clf\n",
    "        out = []\n",
    "        for i in range(n_heads[-1]):\n",
    "            out.append(tf.compat.v1.layers.dense(final_embed, nb_classes, activation=None))\n",
    "        #     out.append(attn_head(h_1, bias_mat=bias_mat,\n",
    "        #                                 out_sz=nb_classes, activation=lambda x: x,\n",
    "        #                                 in_drop=ffd_drop, coef_drop=attn_drop, residual=False))\n",
    "        logits = tf.add_n(out) / n_heads[-1]\n",
    "        # logits_list.append(logits)\n",
    "        logits = tf.expand_dims(logits, axis=0)\n",
    "        if return_coef:\n",
    "            return logits, final_embed, att_val, coef_list\n",
    "        else:\n",
    "            return logits, final_embed, att_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59adf56d",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b3b4f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1,2,3'  # 设置使用GPU1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c4e7990",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()  # 用来对session进行参数配置\n",
    "config.gpu_options.allow_growth = True  # 允许tf自动选择一个存在并且可用的设备来运行操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74b23b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: D:\\PycharmProjects\\GNN_Event_Detection_models/result/Re_HAN_result/offline_result.ckpt\n",
      "Dataset: acm\n",
      "----- Opt. hyperparams -----\n",
      "lr: 0.005\n",
      "l2_coef: 0.001\n",
      "----- Archi. hyperparams -----\n",
      "nb. layers: 1\n",
      "nb. units per layer: [8]\n",
      "nb. attention heads: [8, 1]\n",
      "residual: False\n",
      "nonlinearity: <function elu at 0x000001F86E25EF70>\n",
      "model: <class '__main__.HeteGAT_multi'>\n"
     ]
    }
   ],
   "source": [
    "dataset = 'acm'\n",
    "featype = 'fea'\n",
    "checkpt_file = os.path.abspath(os.path.dirname(os.getcwd())) +\\\n",
    "                            '/result/Re_HAN_result/offline_result.ckpt'\n",
    "print('model: {}'.format(checkpt_file))\n",
    "# training params\n",
    "batch_size = 1\n",
    "nb_epochs = 200\n",
    "patience = 100\n",
    "lr = 0.05  # learning rate\n",
    "l2_coef = 0.005  # weight decay\n",
    "# numbers of hidden units per each attention head in each layer\n",
    "hid_units = [8]\n",
    "n_heads = [8, 1]  # additional entry for the output layer\n",
    "residual = False\n",
    "nonlinearity = tf.nn.elu\n",
    "model = HeteGAT_multi\n",
    "\n",
    "print('Dataset: ' + dataset)\n",
    "print('----- Opt. hyperparams -----')\n",
    "print('lr: ' + str(lr))\n",
    "print('l2_coef: ' + str(l2_coef))\n",
    "print('----- Archi. hyperparams -----')\n",
    "print('nb. layers: ' + str(len(hid_units)))\n",
    "print('nb. units per layer: ' + str(hid_units))\n",
    "print('nb. attention heads: ' + str(n_heads))\n",
    "print('residual: ' + str(residual))\n",
    "print('nonlinearity: ' + str(nonlinearity))\n",
    "print('model: ' + str(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692361ad",
   "metadata": {},
   "source": [
    "## jhy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c8d4888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jhy data\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b08d6c",
   "metadata": {},
   "source": [
    "### sample_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21589aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112af99c",
   "metadata": {},
   "source": [
    "### load_offline_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "930d1608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import torch\n",
    "\n",
    "import os\n",
    "project_path = os.path.abspath(os.path.dirname(os.getcwd()))  # # 获取上级路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "451d81e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\PycharmProjects\\\\GNN_Event_Detection_models'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a3914aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_path = project_path + '/result/FinEvent result/offline dataset'\n",
    "load_embeddings_path = project_path + '/result/FinEvent result/offline result/offline_embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d0fd96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\PycharmProjects\\GNN_Event_Detection_models/result/FinEvent result/offline dataset\n",
      "D:\\PycharmProjects\\GNN_Event_Detection_models/result/FinEvent result/offline result/offline_embeddings\n"
     ]
    }
   ],
   "source": [
    "print(load_data_path)\n",
    "print(load_embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9207a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_offline_data(load_data_path, load_embeddings_path):\n",
    "#     data = sio.loadmat(path)  # load .mat file\n",
    "    '''\n",
    "    全是ndarray\n",
    "    PTP: (3025, 3025)，全是1   <---meta-paths得到的homo 邻接矩阵。\n",
    "    PLP: (3025, 3025)，有0有1，有些向量时相同的\n",
    "    PAP: (3025, 3025)，对角线全是1，其他元素基本是0，很少是1.\n",
    "    feature: (3025, 1870)，由0、1组成。\n",
    "    label: (3025, 3),就3列，1061、965、999\n",
    "    train_idx: (1, 600)，0-2225随机抽取的索引\n",
    "    val_idx: (1, 300)，200-2325之间随机抽取的索引\n",
    "    test_idx: (1, 2125)，300-3024之间随机抽取的索引\n",
    "    '''\n",
    "    \n",
    "    labels = np.load(load_data_path + '/sorted_labels.npy', allow_pickle=True)\n",
    "    # 将单行y转换成label矩阵，一列代表one label\n",
    "    labels_dict = {}\n",
    "    count = 0\n",
    "    for i in range(len(labels)):\n",
    "    #     print(i)\n",
    "        element = labels[i]\n",
    "        if element in labels_dict:\n",
    "            labels[i] = labels_dict[element]\n",
    "        else:\n",
    "            labels_dict[element] = count\n",
    "            labels[i] = count\n",
    "            count += 1\n",
    "    truelabels = np.zeros((labels.shape[0], max(labels) + 1))\n",
    "    for i, label in enumerate(labels):\n",
    "        print(i, label)\n",
    "        truelabels[i][label] = 1\n",
    "\n",
    "    truefeatures = np.load(load_data_path + '/sorted_combined_features_embeddings.npy', allow_pickle=True)\n",
    "    \n",
    "    N = truefeatures.shape[0]\n",
    "    adj_entity = sparse.load_npz(load_data_path + '/s_m_tid_entity_tid_matrix.npz').todense()\n",
    "    adj_entity = np.asarray(adj_entity) - np.eye(N)\n",
    "    adj_userid = sparse.load_npz(load_data_path + '/s_m_tid_userid_tid_matrix.npz').todense()\n",
    "    adj_userid = np.asarray(adj_userid) - np.eye(N)\n",
    "    rownetworks = [adj_entity, adj_userid]  # , data['PTP'] - np.eye(N)]\n",
    "    '''\n",
    "    rownetworks: list: 2。第1个元素，ndarray,(3025, 3025)；第2个元素，ndarray，(3025, 3025)\n",
    "    '''\n",
    "    y = truelabels    # shape为(3025, 3)\n",
    "    train_idx = torch.load(load_embeddings_path + '/block_0/train_mask.pt')  # (1, 600)\n",
    "    train_idx = np.asarray(torch.unsqueeze(train_idx,0))\n",
    "    print(train_idx.shape)\n",
    "    val_idx = torch.load(load_embeddings_path + '/block_0/valid_mask.pt')      # (1, 300)\n",
    "    val_idx = np.asarray(torch.unsqueeze(val_idx,0))\n",
    "    test_idx = torch.load(load_embeddings_path + '/block_0/test_mask.pt')    # (1, 2125)\n",
    "    test_idx = np.asarray(torch.unsqueeze(test_idx,0))\n",
    "\n",
    "    train_mask = sample_mask(train_idx, y.shape[0])  # 3025长度的bool list，train_idx位置为True\n",
    "    val_mask = sample_mask(val_idx, y.shape[0])      # 3025长度的boolean list\n",
    "    test_mask = sample_mask(test_idx, y.shape[0])\n",
    "    \n",
    "    # 提取train、val、test的标签\n",
    "    # 所以，为什么不直接用train_test_split呢？\n",
    "    y_train = np.zeros(y.shape)  # shape为(3025, 3)的zero 列表\n",
    "    y_val = np.zeros(y.shape)\n",
    "    y_test = np.zeros(y.shape)\n",
    "    y_train[train_mask, :] = y[train_mask, :]  # 取出train_idx为true的label，放入y_train，y_train其余位置为0\n",
    "    y_val[val_mask, :] = y[val_mask, :]\n",
    "    y_test[test_mask, :] = y[test_mask, :]\n",
    "\n",
    "    # return selected_idx, selected_idx_2\n",
    "    print('y_train:{}, y_val:{}, y_test:{}, train_idx:{}, val_idx:{}, test_idx:{}'.format(y_train.shape,\n",
    "                                                                                          y_val.shape,\n",
    "                                                                                          y_test.shape,\n",
    "                                                                                          train_idx.shape,\n",
    "                                                                                          val_idx.shape,\n",
    "                                                                                          test_idx.shape))\n",
    "    truefeatures_list = [truefeatures, truefeatures, truefeatures]   # truefeatures: (3025, 1870)\n",
    "    return rownetworks, truefeatures_list, y_train, y_val, y_test, train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0dd88a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8380)\n",
      "y_train:(11971, 1), y_val:(11971, 1), y_test:(11971, 1), train_idx:(1, 8380), val_idx:(1, 2394), test_idx:(1, 1197)\n"
     ]
    }
   ],
   "source": [
    "# use adj_list as fea_list, have a try~\n",
    "adj_list, fea_list, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_offline_data(load_data_path, load_embeddings_path)\n",
    "if featype == 'adj':\n",
    "    fea_list = adj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "56394450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11971, 302)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "847f4938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.52512252e+00, -1.28044748e+00, -1.67360997e+00, ...,\n",
       "         -1.10073507e+00,  4.11920000e+00,  1.50462963e-04],\n",
       "        [-8.79233256e-02, -6.25963390e-01, -9.90003288e-01, ...,\n",
       "          4.68896657e-01,  4.11920000e+00,  4.97685185e-04],\n",
       "        [-7.38342285e-01, -2.31036329e+00, -1.53008020e+00, ...,\n",
       "          3.20886701e-01,  4.11920000e+00,  5.78703704e-04],\n",
       "        ...,\n",
       "        [ 8.28718364e-01,  1.84172881e+00, -1.79105675e+00, ...,\n",
       "          7.84933940e-02,  4.11950000e+00,  9.96261574e-01],\n",
       "        [-3.74932468e-01, -4.66887876e-02, -1.77780282e+00, ...,\n",
       "          6.00812554e-01,  4.11950000e+00,  9.97430556e-01],\n",
       "        [-3.72165012e+00, -1.18709993e+00,  3.44226480e+00, ...,\n",
       "         -4.23700523e+00,  4.11950000e+00,  9.99652778e-01]]),\n",
       " array([[-1.52512252e+00, -1.28044748e+00, -1.67360997e+00, ...,\n",
       "         -1.10073507e+00,  4.11920000e+00,  1.50462963e-04],\n",
       "        [-8.79233256e-02, -6.25963390e-01, -9.90003288e-01, ...,\n",
       "          4.68896657e-01,  4.11920000e+00,  4.97685185e-04],\n",
       "        [-7.38342285e-01, -2.31036329e+00, -1.53008020e+00, ...,\n",
       "          3.20886701e-01,  4.11920000e+00,  5.78703704e-04],\n",
       "        ...,\n",
       "        [ 8.28718364e-01,  1.84172881e+00, -1.79105675e+00, ...,\n",
       "          7.84933940e-02,  4.11950000e+00,  9.96261574e-01],\n",
       "        [-3.74932468e-01, -4.66887876e-02, -1.77780282e+00, ...,\n",
       "          6.00812554e-01,  4.11950000e+00,  9.97430556e-01],\n",
       "        [-3.72165012e+00, -1.18709993e+00,  3.44226480e+00, ...,\n",
       "         -4.23700523e+00,  4.11950000e+00,  9.99652778e-01]]),\n",
       " array([[-1.52512252e+00, -1.28044748e+00, -1.67360997e+00, ...,\n",
       "         -1.10073507e+00,  4.11920000e+00,  1.50462963e-04],\n",
       "        [-8.79233256e-02, -6.25963390e-01, -9.90003288e-01, ...,\n",
       "          4.68896657e-01,  4.11920000e+00,  4.97685185e-04],\n",
       "        [-7.38342285e-01, -2.31036329e+00, -1.53008020e+00, ...,\n",
       "          3.20886701e-01,  4.11920000e+00,  5.78703704e-04],\n",
       "        ...,\n",
       "        [ 8.28718364e-01,  1.84172881e+00, -1.79105675e+00, ...,\n",
       "          7.84933940e-02,  4.11950000e+00,  9.96261574e-01],\n",
       "        [-3.74932468e-01, -4.66887876e-02, -1.77780282e+00, ...,\n",
       "          6.00812554e-01,  4.11950000e+00,  9.97430556e-01],\n",
       "        [-3.72165012e+00, -1.18709993e+00,  3.44226480e+00, ...,\n",
       "         -4.23700523e+00,  4.11950000e+00,  9.99652778e-01]])]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f0444ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ..., False,  True, False])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3143462f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[394.],\n",
       "       [394.],\n",
       "       [394.],\n",
       "       ...,\n",
       "       [  0.],\n",
       "       [279.],\n",
       "       [  0.]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad57b9af",
   "metadata": {},
   "source": [
    "## run_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0fa80a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e5d3d",
   "metadata": {},
   "source": [
    "### add_data_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8385aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truefeatures: (3025, 1870)\n",
    "nb_nodes = fea_list[0].shape[0]  # 3025\n",
    "ft_size = fea_list[0].shape[1]   # 1870\n",
    "nb_classes = y_train.shape[1]    # 3\n",
    "\n",
    "# adj = adj.todense()\n",
    "\n",
    "# features = features[np.newaxis]  # [1, nb_node, ft_size]\n",
    "fea_list = [fea[np.newaxis] for fea in fea_list]  # np.newaxis行增加一个新的维度\n",
    "'''\n",
    "fea_list:list:3。第一个元素，ndarray, (1, 3025, 1870)\n",
    "'''\n",
    "adj_list = [adj[np.newaxis] for adj in adj_list]  # adj_list: 2. 单个元素(1, 3025, 3025)\n",
    "y_train = y_train[np.newaxis]  # ndarray: (1, 3025, 3)\n",
    "y_val = y_val[np.newaxis]      # ndarray: (1, 3025, 3)\n",
    "y_test = y_test[np.newaxis]    # ndarray: (1, 3025, 3)\n",
    "train_mask = train_mask[np.newaxis]  # ndarray(1, 3025)\n",
    "val_mask = val_mask[np.newaxis]      # ndarray(1, 3025)\n",
    "test_mask = test_mask[np.newaxis]    # ndarray(1, 3025)\n",
    "\n",
    "biases_list = [adj_to_bias(adj, [nb_nodes], nhood=1) for adj in adj_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "af0066ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 11971, 302)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_list[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b402472f",
   "metadata": {},
   "source": [
    "### build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6dec9169",
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_836\\3938853343.py:20: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
      "  features_fts = tf.compat.v1.layers.conv1d(features, out_sz, 1, use_bias=False)  # 一维卷积操作, out: (1, 3025, 8)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_836\\3938853343.py:22: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
      "  f_1 = tf.compat.v1.layers.conv1d(features_fts, 1, 1)  # (1, 3025, 1)\n",
      "C:\\Users\\yysgz\\AppData\\Local\\Temp\\ipykernel_836\\3938853343.py:23: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
      "  f_2 = tf.compat.v1.layers.conv1d(features_fts, 1, 1)  # (1, 3025, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 64 and 11971 for '{{node v/Tensordot/MatMul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](v/Tensordot/Reshape, v/Tensordot/ReadVariableOp)' with input shapes: [2394200,64], [11971,128].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [145]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     is_train \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mplaceholder(dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mbool, shape\u001b[38;5;241m=\u001b[39m(), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_train\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# # tensor, ()\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# forward；model = HeteGAT_multi\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m logits, final_embedding, att_val \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mftr_in_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# list:3, tensor（1， 3025， 1870）\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mnb_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# 3\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mnb_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# 3025\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# bool\u001b[39;49;00m\n\u001b[0;32m     26\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mattn_drop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# tensor, ()\u001b[39;49;00m\n\u001b[0;32m     27\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mffd_drop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# tensor, ()\u001b[39;49;00m\n\u001b[0;32m     28\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mbias_mat_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias_in_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# list:2, tensor(1, 3025, 3025)\u001b[39;49;00m\n\u001b[0;32m     29\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mhid_units\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhid_units\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# hid_units:[8]\u001b[39;49;00m\n\u001b[0;32m     30\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# n_heads: [8, 1]\u001b[39;49;00m\n\u001b[0;32m     31\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mresidual\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# residual: False\u001b[39;49;00m\n\u001b[0;32m     32\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnonlinearity\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# nonlinearity:tf.nn.elu\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# cal masked_loss\u001b[39;00m\n\u001b[0;32m     35\u001b[0m log_resh \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(logits, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, nb_classes])  \u001b[38;5;66;03m# （3025， 3）\u001b[39;00m\n",
      "Input \u001b[1;32mIn [69]\u001b[0m, in \u001b[0;36mHeteGAT_multi.inference\u001b[1;34m(ftr_in_list, nb_classes, nb_nodes, training, attn_drop, ffd_drop, bias_mat_list, hid_units, n_heads, activation, residual, mp_att_size)\u001b[0m\n\u001b[0;32m     43\u001b[0m multi_embed \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat(embed_list, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)   \u001b[38;5;66;03m# tensor, (3025, 2, 64)\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# attention输出：tensor(3025, 64)、softmax概率\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m final_embed, att_val \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleAttLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmulti_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mmp_att_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mtime_major\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mreturn_alphas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m out \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_heads[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):  \u001b[38;5;66;03m# 1\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# 用于添加一个全连接层(input, output) -> (3025, 3)\u001b[39;00m\n",
      "Input \u001b[1;32mIn [60]\u001b[0m, in \u001b[0;36mSimpleAttLayer\u001b[1;34m(inputs, attention_size, time_major, return_alphas)\u001b[0m\n\u001b[0;32m     22\u001b[0m u_omega \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mVariable(tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mrandom_normal([attention_size], stddev\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m))               \u001b[38;5;66;03m# (128, )\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Applying fully connected layer with non-linear activation to each of the B*T timestamps;\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m#  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     v \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtanh(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensordot\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_omega\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b_omega)   \u001b[38;5;66;03m# (3025, 2, 128)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# For each of the timestamps its vector of size A from `v` is reduced with `u` vector\u001b[39;00m\n\u001b[0;32m     30\u001b[0m vu \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtensordot(v, u_omega, axes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvu\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# (B,T) shape   tensor, (3025, 2)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\ops.py:1967\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[0;32m   1964\u001b[0m   c_op \u001b[38;5;241m=\u001b[39m pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_FinishOperation(op_desc)\n\u001b[0;32m   1965\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1966\u001b[0m   \u001b[38;5;66;03m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[1;32m-> 1967\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(e\u001b[38;5;241m.\u001b[39mmessage)\n\u001b[0;32m   1969\u001b[0m \u001b[38;5;66;03m# Record the current Python stack trace as the creating stacktrace of this\u001b[39;00m\n\u001b[0;32m   1970\u001b[0m \u001b[38;5;66;03m# TF_Operation.\u001b[39;00m\n\u001b[0;32m   1971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extract_traceback:\n",
      "\u001b[1;31mValueError\u001b[0m: Dimensions must be equal, but are 64 and 11971 for '{{node v/Tensordot/MatMul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](v/Tensordot/Reshape, v/Tensordot/ReadVariableOp)' with input shapes: [2394200,64], [11971,128]."
     ]
    }
   ],
   "source": [
    "print('build graph...')\n",
    "with tf.Graph().as_default():  # 创建一个新的计算图\n",
    "    with tf.name_scope('input'):  # 创建一个上下文管理器\n",
    "        ftr_in_list = [tf.compat.v1.placeholder(dtype=tf.float32,  # 占位符，提前分配必要的内存\n",
    "                                      shape=(batch_size, nb_nodes, ft_size),  # batch_size:1, nb_nodes:3025, fea_size: 1870\n",
    "                                      name='ftr_in_{}'.format(i))\n",
    "                       for i in range(len(fea_list))]  # fea_list,长度为3，内部单个元素，(1, 3025, 1870)\n",
    "        bias_in_list = [tf.compat.v1.placeholder(dtype=tf.float32,\n",
    "                                       shape=(batch_size, nb_nodes, nb_nodes),\n",
    "                                       name='bias_in_{}'.format(i))\n",
    "                        for i in range(len(biases_list))]  # 邻接矩阵转换成的biases_list: 2. 单个元素占位符tensor, (1, 3025, 3025)\n",
    "        lbl_in = tf.compat.v1.placeholder(dtype=tf.int32, \n",
    "                                        shape=(batch_size, nb_nodes, nb_classes),   # tensor, nb_classes: 3\n",
    "                                        name='lbl_in')   \n",
    "        msk_in = tf.compat.v1.placeholder(dtype=tf.int32, \n",
    "                                        shape=(batch_size, nb_nodes),  # tensor, (1, 3025)\n",
    "                                        name='msk_in')\n",
    "        attn_drop = tf.compat.v1.placeholder(dtype=tf.float32, shape=(), name='attn_drop')  # tensor, ()\n",
    "        ffd_drop = tf.compat.v1.placeholder(dtype=tf.float32, shape=(), name='ffd_drop')  # # tensor, ()\n",
    "        is_train = tf.compat.v1.placeholder(dtype=tf.bool, shape=(), name='is_train')  # # tensor, ()\n",
    "    # forward；model = HeteGAT_multi\n",
    "    logits, final_embedding, att_val = model.inference(ftr_in_list,  # list:3, tensor（1， 3025， 1870）\n",
    "                                                       nb_classes,   # 3\n",
    "                                                       nb_nodes,     # 3025\n",
    "                                                       is_train,     # bool\n",
    "                                                       attn_drop,    # tensor, ()\n",
    "                                                       ffd_drop,     # tensor, ()\n",
    "                                                       bias_mat_list=bias_in_list,  # list:2, tensor(1, 3025, 3025)\n",
    "                                                       hid_units=hid_units,   # hid_units:[8]\n",
    "                                                       n_heads=n_heads,       # n_heads: [8, 1]\n",
    "                                                       residual=residual,     # residual: False\n",
    "                                                       activation=nonlinearity)  # nonlinearity:tf.nn.elu\n",
    "\n",
    "    # cal masked_loss\n",
    "    log_resh = tf.reshape(logits, [-1, nb_classes])  # （3025， 3）\n",
    "    lab_resh = tf.reshape(lbl_in, [-1, nb_classes])  # （3025， 3）\n",
    "    msk_resh = tf.reshape(msk_in, [-1])              # mask，（3025， ）\n",
    "    loss = model.masked_softmax_cross_entropy(log_resh, lab_resh, msk_resh)  # 占位符计算softmax cross_entropy based on (pred, y)\n",
    "    accuracy = model.masked_accuracy(log_resh, lab_resh, msk_resh)           # 计算accuracy\n",
    "    # optimzie\n",
    "    train_op = model.training(loss, lr, l2_coef)  # lr = 0.005、l2_coef = 0.001\n",
    "\n",
    "    saver = tf.compat.v1.train.Saver()  # 用于保存模型\n",
    "\n",
    "    init_op = tf.group(tf.compat.v1.global_variables_initializer(),  # 全局变量初始化；group组合多个operation\n",
    "                       tf.compat.v1.local_variables_initializer())\n",
    "\n",
    "    vlss_mn = np.inf\n",
    "    vacc_mx = 0.0\n",
    "    curr_step = 0\n",
    "\n",
    "    with tf.compat.v1.Session(config=config) as sess:  # 创建session\n",
    "        sess.run(init_op)\n",
    "\n",
    "        train_loss_avg = 0\n",
    "        train_acc_avg = 0\n",
    "        val_loss_avg = 0\n",
    "        val_acc_avg = 0\n",
    "\n",
    "        for epoch in range(nb_epochs):  # 200\n",
    "            tr_step = 0\n",
    "           \n",
    "            tr_size = fea_list[0].shape[0]\n",
    "            # ================   training    ============\n",
    "            while tr_step * batch_size < tr_size:\n",
    "                # feature,占位符内存已经分配完毕，fea_list是真实数据，输入进行训练模型\n",
    "                fd1 = {i: d[tr_step * batch_size:(tr_step + 1) * batch_size]  # dict:3. 每个元素tensor, (1, 3025, 1870)\n",
    "                       for i, d in zip(ftr_in_list, fea_list)}\n",
    "                # bias\n",
    "                fd2 = {i: d[tr_step * batch_size:(tr_step + 1) * batch_size]  # dict: 2. 每个元素tensor, (1, 3025, 3025)\n",
    "                       for i, d in zip(bias_in_list, biases_list)}\n",
    "                # other params\n",
    "                fd3 = {lbl_in: y_train[tr_step * batch_size:(tr_step + 1) * batch_size],\n",
    "                       msk_in: train_mask[tr_step * batch_size:(tr_step + 1) * batch_size],\n",
    "                       is_train: True,\n",
    "                       attn_drop: 0.6,\n",
    "                       ffd_drop: 0.6}\n",
    "                fd = fd1\n",
    "                fd.update(fd2)  # 字典update方法\n",
    "                fd.update(fd3)  # 获得字典形式的所有数据、参数\n",
    "                # training操作：更新权重；计算loss；计算accuracy；attention概率\n",
    "                _, loss_value_tr, acc_tr, att_val_train = sess.run([train_op, loss, accuracy, att_val],\n",
    "                                                                   feed_dict=fd)\n",
    "                train_loss_avg += loss_value_tr\n",
    "                train_acc_avg += acc_tr\n",
    "                tr_step += 1\n",
    "\n",
    "            vl_step = 0\n",
    "            vl_size = fea_list[0].shape[0]\n",
    "            # =============   val       =================\n",
    "            while vl_step * batch_size < vl_size:\n",
    "                # fd1 = {ftr_in: features[vl_step * batch_size:(vl_step + 1) * batch_size]}\n",
    "                fd1 = {i: d[vl_step * batch_size:(vl_step + 1) * batch_size]\n",
    "                       for i, d in zip(ftr_in_list, fea_list)}\n",
    "                fd2 = {i: d[vl_step * batch_size:(vl_step + 1) * batch_size]\n",
    "                       for i, d in zip(bias_in_list, biases_list)}\n",
    "                fd3 = {lbl_in: y_val[vl_step * batch_size:(vl_step + 1) * batch_size],\n",
    "                       msk_in: val_mask[vl_step * batch_size:(vl_step + 1) * batch_size],\n",
    "                       is_train: False,\n",
    "                       attn_drop: 0.0,\n",
    "                       ffd_drop: 0.0}\n",
    "          \n",
    "                fd = fd1\n",
    "                fd.update(fd2)\n",
    "                fd.update(fd3)\n",
    "                loss_value_vl, acc_vl = sess.run([loss, accuracy],\n",
    "                                                 feed_dict=fd)\n",
    "                val_loss_avg += loss_value_vl\n",
    "                val_acc_avg += acc_vl\n",
    "                vl_step += 1\n",
    "            # import pdb; pdb.set_trace()\n",
    "            print('Epoch: {}, att_val: {}'.format(epoch, np.mean(att_val_train, axis=0)))\n",
    "            print('Training: loss = %.5f, acc = %.5f | Val: loss = %.5f, acc = %.5f' %\n",
    "                  (train_loss_avg / tr_step, train_acc_avg / tr_step,\n",
    "                   val_loss_avg / vl_step, val_acc_avg / vl_step))\n",
    "            \n",
    "            # =============   judging  =================\n",
    "            if val_acc_avg / vl_step >= vacc_mx or val_loss_avg / vl_step <= vlss_mn:\n",
    "                if val_acc_avg / vl_step >= vacc_mx and val_loss_avg / vl_step <= vlss_mn:\n",
    "                    vacc_early_model = val_acc_avg / vl_step\n",
    "                    vlss_early_model = val_loss_avg / vl_step\n",
    "                    saver.save(sess, checkpt_file)\n",
    "                vacc_mx = np.max((val_acc_avg / vl_step, vacc_mx))\n",
    "                vlss_mn = np.min((val_loss_avg / vl_step, vlss_mn))\n",
    "                curr_step = 0\n",
    "            else:\n",
    "                curr_step += 1\n",
    "                if curr_step == patience:\n",
    "                    print('Early stop! Min loss: ', vlss_mn,\n",
    "                          ', Max accuracy: ', vacc_mx)\n",
    "                    print('Early stop model validation loss: ',\n",
    "                          vlss_early_model, ', accuracy: ', vacc_early_model)\n",
    "                    break\n",
    "\n",
    "            train_loss_avg = 0\n",
    "            train_acc_avg = 0\n",
    "            val_loss_avg = 0\n",
    "            val_acc_avg = 0\n",
    "        \n",
    "        # loading model params\n",
    "        saver.restore(sess, checkpt_file)\n",
    "        print('load model from : {}'.format(checkpt_file))\n",
    "        ts_size = fea_list[0].shape[0]\n",
    "        ts_step = 0\n",
    "        ts_loss = 0.0\n",
    "        ts_acc = 0.0\n",
    "        \n",
    "        # ============= testing =================\n",
    "        while ts_step * batch_size < ts_size:\n",
    "            # fd1 = {ftr_in: features[ts_step * batch_size:(ts_step + 1) * batch_size]}\n",
    "            fd1 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                   for i, d in zip(ftr_in_list, fea_list)}\n",
    "            fd2 = {i: d[ts_step * batch_size:(ts_step + 1) * batch_size]\n",
    "                   for i, d in zip(bias_in_list, biases_list)}\n",
    "            fd3 = {lbl_in: y_test[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "                   msk_in: test_mask[ts_step * batch_size:(ts_step + 1) * batch_size],\n",
    "            \n",
    "                   is_train: False,\n",
    "                   attn_drop: 0.0,\n",
    "                   ffd_drop: 0.0}\n",
    "        \n",
    "            fd = fd1\n",
    "            fd.update(fd2)\n",
    "            fd.update(fd3)\n",
    "            loss_value_ts, acc_ts, jhy_final_embedding = sess.run([loss, accuracy, final_embedding],\n",
    "                                                                  feed_dict=fd)\n",
    "            ts_loss += loss_value_ts\n",
    "            ts_acc += acc_ts\n",
    "            ts_step += 1\n",
    "\n",
    "        print('Test loss:', ts_loss / ts_step,\n",
    "              '; Test accuracy:', ts_acc / ts_step)\n",
    "\n",
    "        print('start knn, kmean.....')\n",
    "        xx = np.expand_dims(jhy_final_embedding, axis=0)[test_mask]\n",
    "  \n",
    "        from numpy import linalg as LA\n",
    "\n",
    "        # xx = xx / LA.norm(xx, axis=1)\n",
    "        yy = y_test[test_mask]\n",
    "\n",
    "        print('xx: {}, yy: {}'.format(xx.shape, yy.shape))\n",
    "\n",
    "#         my_KNN(xx, yy)\n",
    "        my_Kmeans(xx, yy)\n",
    "\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e09b1a8",
   "metadata": {},
   "source": [
    "# baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f10b694",
   "metadata": {},
   "source": [
    "## load_tweeter_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f313361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "project_path = os.path.abspath(os.path.dirname(os.getcwd()))  # # 获取上级路径\n",
    "\n",
    "load_path = project_path + '/data/FinEvent_datasets/raw dataset/'\n",
    "save_path = project_path + '/result/FinEvent result/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ac414d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\PycharmProjects\\GNN_Event_Detection_models/data/FinEvent_datasets/raw dataset/\n",
      "D:\\PycharmProjects\\GNN_Event_Detection_models/result/FinEvent result/\n"
     ]
    }
   ],
   "source": [
    "print(load_path)\n",
    "print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75b3093e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n",
      "Data converted to dataframe.\n",
      "(11971, 18)\n",
      "89\n",
      "10905\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# load data (68841 tweets, multiclasses filtered)\n",
    "p_part1 = load_path + '68841_tweets_multiclasses_filtered_0722_part1.npy'\n",
    "p_part2 = load_path + '68841_tweets_multiclasses_filtered_0722_part2.npy'\n",
    "# allow_pickle: 可选，布尔值，允许使用 Python pickles 保存对象数组，Python 中的 pickle 用于在保存到磁盘文件或从磁盘文件读取之前，对对象进行序列化和反序列化。\n",
    "np_part1 = np.load(p_part1, allow_pickle=True)   # (35000, 16)\n",
    "np_part2 = np.load(p_part2, allow_pickle=True)   # (33841, 16)\n",
    "\n",
    "np_tweets = np.concatenate((np_part1, np_part2), axis=0)  # (68841, 16)\n",
    "print('Data loaded.')\n",
    "\n",
    "df = pd.DataFrame(data=np_tweets, columns=['event_id', 'tweet_id', 'text', 'user_id', 'created_at', 'user_loc',\n",
    "                                      'place_type', 'place_full_name', 'place_country_code', 'hashtags',\n",
    "                                      'user_mentions', 'image_urls', 'entities', 'words', 'filtered_words', 'sampled_words'])\n",
    "print('Data converted to dataframe.')\n",
    "# sort date by time\n",
    "df = df.sort_values(by='created_at').reset_index(drop=True)\n",
    "\n",
    "# append date\n",
    "df['date'] = [d.date() for d in df['created_at']]\n",
    "# 因为graph太大，爆了内存，所以取4天的twitter data做demo，后面用nci server\n",
    "init_day = df.loc[0, 'date']\n",
    "df = df[(df['date']>= init_day) & (df['date']<= init_day + datetime.timedelta(days=3))].reset_index() # (11971, 18)\n",
    "print(df.shape)\n",
    "print(df.event_id.nunique())\n",
    "print(df.user_id.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "256e85c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11971, 18)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea407ee",
   "metadata": {},
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f3ad24",
   "metadata": {},
   "source": [
    "### bert_model_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "541b786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28122975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74760fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer('hello world', return_tensors='pt')\n",
    "outputs = bert_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f66e273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][0][0][:100].shape  # 取第1层，也可以取别的层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85c49a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>event_id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>user_loc</th>\n",
       "      <th>place_type</th>\n",
       "      <th>place_full_name</th>\n",
       "      <th>place_country_code</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>image_urls</th>\n",
       "      <th>entities</th>\n",
       "      <th>words</th>\n",
       "      <th>filtered_words</th>\n",
       "      <th>sampled_words</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>394</td>\n",
       "      <td>255819992157786112</td>\n",
       "      <td>HipHop awards bout to be live!!</td>\n",
       "      <td>250870763</td>\n",
       "      <td>2012-10-10 00:00:13</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[award, live, bout, hiphop]</td>\n",
       "      <td>[award, live, bout, hiphop]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2012-10-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>394</td>\n",
       "      <td>255820118095978496</td>\n",
       "      <td>HIPHOP AWARDS TIME!</td>\n",
       "      <td>28026779</td>\n",
       "      <td>2012-10-10 00:00:43</td>\n",
       "      <td>SoundCloud/RaRaSupaStar</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[HIPHOP, AWARDS, time]</td>\n",
       "      <td>[hiphop, awards, time]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2012-10-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>394</td>\n",
       "      <td>255820147489636353</td>\n",
       "      <td>Bet hiphop awards</td>\n",
       "      <td>566825483</td>\n",
       "      <td>2012-10-10 00:00:50</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(Bet, GPE)]</td>\n",
       "      <td>[award, bet, hiphop]</td>\n",
       "      <td>[award, bet, hiphop]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2012-10-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>394</td>\n",
       "      <td>255820164023595008</td>\n",
       "      <td>BET HipHop awards is on!!!</td>\n",
       "      <td>197834311</td>\n",
       "      <td>2012-10-10 00:00:54</td>\n",
       "      <td>Saint Lucia ☀️🌴🇱🇨</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[HipHop, BET, award]</td>\n",
       "      <td>[hiphop, bet, award]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2012-10-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>394</td>\n",
       "      <td>255820180884701184</td>\n",
       "      <td>Watchin Da BET Hiphop Awards</td>\n",
       "      <td>439490861</td>\n",
       "      <td>2012-10-10 00:00:58</td>\n",
       "      <td>Michigan, USA</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Hiphop, Watchin, Awards, Da, BET]</td>\n",
       "      <td>[hiphop, watchin, awards, da, bet]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2012-10-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index event_id            tweet_id                             text  \\\n",
       "0      0      394  255819992157786112  HipHop awards bout to be live!!   \n",
       "1      1      394  255820118095978496              HIPHOP AWARDS TIME!   \n",
       "2      2      394  255820147489636353                Bet hiphop awards   \n",
       "3      3      394  255820164023595008       BET HipHop awards is on!!!   \n",
       "4      4      394  255820180884701184     Watchin Da BET Hiphop Awards   \n",
       "\n",
       "     user_id          created_at                 user_loc place_type  \\\n",
       "0  250870763 2012-10-10 00:00:13                                       \n",
       "1   28026779 2012-10-10 00:00:43  SoundCloud/RaRaSupaStar              \n",
       "2  566825483 2012-10-10 00:00:50                                       \n",
       "3  197834311 2012-10-10 00:00:54        Saint Lucia ☀️🌴🇱🇨              \n",
       "4  439490861 2012-10-10 00:00:58            Michigan, USA              \n",
       "\n",
       "  place_full_name place_country_code hashtags user_mentions image_urls  \\\n",
       "0                                          []            []         []   \n",
       "1                                          []            []         []   \n",
       "2                                          []            []         []   \n",
       "3                                          []            []         []   \n",
       "4                                          []            []         []   \n",
       "\n",
       "       entities                               words  \\\n",
       "0            []         [award, live, bout, hiphop]   \n",
       "1            []              [HIPHOP, AWARDS, time]   \n",
       "2  [(Bet, GPE)]                [award, bet, hiphop]   \n",
       "3            []                [HipHop, BET, award]   \n",
       "4            []  [Hiphop, Watchin, Awards, Da, BET]   \n",
       "\n",
       "                       filtered_words sampled_words        date  \n",
       "0         [award, live, bout, hiphop]            []  2012-10-10  \n",
       "1              [hiphop, awards, time]            []  2012-10-10  \n",
       "2                [award, bet, hiphop]            []  2012-10-10  \n",
       "3                [hiphop, bet, award]            []  2012-10-10  \n",
       "4  [hiphop, watchin, awards, da, bet]            []  2012-10-10  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392c8df8",
   "metadata": {},
   "source": [
    "### bert_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ea3157f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11971, 18)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d161874b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "1\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "2\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "3\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "4\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "5\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "6\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "7\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "8\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "9\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "10\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "11\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "12\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "13\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "14\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "15\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "16\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "17\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "18\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "19\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "20\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "21\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "22\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "23\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "24\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "25\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "26\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "27\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "28\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "29\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "30\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "31\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "32\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "33\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "34\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "35\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "36\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "37\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "38\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "39\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "40\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "41\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "42\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "43\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "44\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "45\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "46\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "47\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "48\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "49\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "50\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "51\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "52\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "53\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "54\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "55\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "56\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "57\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "58\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "59\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "60\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "61\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "62\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "63\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "64\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "65\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "66\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "67\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "68\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "69\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "70\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "71\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "72\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "73\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "74\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "75\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "76\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "77\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "78\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "79\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "80\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "81\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "82\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "83\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "84\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "85\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "86\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "87\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "88\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "89\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "90\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "91\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "92\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "93\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "94\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "95\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "96\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "97\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "98\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "99\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "100\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "101\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "102\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "103\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "104\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "105\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "106\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "107\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "108\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "109\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "110\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "111\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "112\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "113\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "114\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "115\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "116\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "117\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n",
      "118\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 768)\n",
      "(100,)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [136]\u001b[0m, in \u001b[0;36m<cell line: 61>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m     total_ami\u001b[38;5;241m.\u001b[39mappend(batch_ami)\n\u001b[0;32m     59\u001b[0m     total_ari\u001b[38;5;241m.\u001b[39mappend(batch_ari)\n\u001b[1;32m---> 61\u001b[0m metric_nmi \u001b[38;5;241m=\u001b[39m \u001b[43mtotal_nmi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m()\n\u001b[0;32m     62\u001b[0m metric_ami \u001b[38;5;241m=\u001b[39m total_ami\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     63\u001b[0m metric_ari \u001b[38;5;241m=\u001b[39m total_ari\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'mean'"
     ]
    }
   ],
   "source": [
    "# 文件太大，爆内存df['bert_embeddings'] = df.text.apply(lambda x: bert_model(**tokenizer(x, return_tensors='pt'))[0])\n",
    "# solution: mini-batch\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from sklearn.cluster import DBSCAN,KMeans\n",
    "# NMI, AMI, ARI\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "# 生成随机序列\n",
    "random_seq = torch.randperm(df.shape[0])\n",
    "\n",
    "# tran_indies, test_indies = train_test_split(random_seq, test_size=0.2)\n",
    "total_nmi = []\n",
    "total_ami = []\n",
    "total_ari = []\n",
    "batch_size = 100\n",
    "batch_nums = int(df.shape[0] / batch_size)\n",
    "# 因为聚类指标无法训练，只能fit和fit_predict()，所以取mini-batch指标平均值\n",
    "for i in range(batch_nums):\n",
    "    print(i)\n",
    "    if i == batch_nums-1:\n",
    "        mini_embed_indies = random_seq[i*batch_size:]\n",
    "    else:\n",
    "        mini_embed_indies = random_seq[i*batch_size:(i+1)*batch_size]\n",
    "    df_mini_words = df.loc[embed_indies, 'filtered_words'].reset_index()\n",
    "    embedding_lists = []\n",
    "    for j in range(len(df_mini_words)):\n",
    "        inputs = df_mini_words.loc[j,'filtered_words']\n",
    "        inputs_sentence = ' '.join(inputs)\n",
    "        inputs_embedding = bert_model(**tokenizer(inputs_sentence, return_tensors='pt'))[0][0][0]\n",
    "        embedding_lists.append(inputs_embedding.detach().numpy())\n",
    "#         data_mini_x = torch.cat((data_mini_x, inputs_embedding), dim=0)\n",
    "\n",
    "#     data_mini_x = [bert_model(**tokenizer(' '.join(i), return_tensors='pt'))[0][0][0]\n",
    "#                    for i in df_mini_words]\n",
    "#     data_mini_x = torch.unsqueeze(data_mini_x,1)\n",
    "#     break\n",
    "    data_mini_x = np.array(embedding_lists)\n",
    "    print(type(data_mini_x))\n",
    "    print(data_mini_x.shape)\n",
    "\n",
    "    data_mini_y = np.array(df.loc[embed_indies, 'event_id'])\n",
    "#     data_mini_y = np.expand_dims(data_mini_y, 1)\n",
    "    print(data_mini_y.shape)\n",
    "    # 拟合模型\n",
    "    n_classes = len(np.unique(data_mini_y))\n",
    "    kmeans_model = KMeans(n_clusters=n_classes).fit(data_mini_x, data_mini_y)  # 聚类模型\n",
    "#     pred_y = kmeans_model.labels_\n",
    "    pred_y = kmeans_model.predict(data_mini_x)\n",
    "    # 计算clustering evaluation指标\n",
    "    batch_nmi = normalized_mutual_info_score(data_mini_y, pred_y)\n",
    "    batch_ami = adjusted_mutual_info_score(data_mini_y, pred_y)\n",
    "    batch_ari = adjusted_rand_score(data_mini_y, pred_y)\n",
    "    \n",
    "    total_nmi.append(batch_nmi)\n",
    "    total_ami.append(batch_ami)\n",
    "    total_ari.append(batch_ari)\n",
    "\n",
    "metric_nmi = np.mean(total_nmi\n",
    "metric_ami = np.mean(total_ami)\n",
    "metric_ari = np.mean(total_ari)\n",
    "print('metric_nmi:' + metric_nmi, 'metric_ami:' + metric_ami, 'metric_ari:' + metric_ari)\n",
    "print('embeddings finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f22b4df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_nmi = np.mean(total_nmi)\n",
    "metric_ami = np.mean(total_ami)\n",
    "metric_ari = np.mean(total_ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "812d1218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7517083348140321"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_nmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2718f611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2645850173310259"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_ami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "15f360ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1392537309157387"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_ari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e8ddc7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mini_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "307958f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "10ea832d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([240, 1, 279, 419, 502, 91, 78, 1, 278, 18, 91, 2, 1, 1, 1, 282, 91,\n",
       "       243, 1, 1, 1, 18, 21, 487, 1, 1, 427, 3, 279, 0, 384, 472, 477,\n",
       "       487, 1, 421, 243, 88, 246, 3, 243, 278, 75, 1, 394, 1, 386, 242, 2,\n",
       "       75, 502, 241, 243, 19, 422, 419, 1, 2, 246, 475, 18, 280, 21, 83,\n",
       "       1, 1, 106, 1, 504, 474, 3, 3, 240, 238, 240, 2, 421, 383, 1, 80,\n",
       "       423, 1, 94, 246, 19, 3, 85, 1, 278, 1, 502, 422, 429, 18, 2, 475,\n",
       "       279, 425, 1, 3], dtype=object)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mini_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fb5b14bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=2)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])\n",
    "k_means = KMeans(2)\n",
    "k_means.fit(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ffd9b585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. ,  2. ],\n",
       "       [ 1.5,  1.8],\n",
       "       [ 5. ,  8. ],\n",
       "       [ 8. ,  8. ],\n",
       "       [ 1. ,  0.6],\n",
       "       [ 9. , 11. ]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d61f752a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 2)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2bfc9ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "37086f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.28958654, -0.12949194,  0.00678096, ..., -0.07712209,\n",
       "         0.05935615,  0.4994283 ],\n",
       "       [-0.24104664, -0.00732547,  0.04749957, ..., -0.20405638,\n",
       "         0.26195958,  0.16743796],\n",
       "       [-0.3300131 , -0.00179507, -0.3481554 , ..., -0.05225424,\n",
       "         0.32856098,  0.43455335],\n",
       "       ...,\n",
       "       [-0.50517625,  0.07986608, -0.5771193 , ..., -0.47181144,\n",
       "         0.09768105,  0.49552682],\n",
       "       [-0.14235994,  0.09129456, -0.01598077, ..., -0.02290452,\n",
       "         0.27011776,  0.2527727 ],\n",
       "       [-0.03415905,  0.35531443, -0.10913993, ..., -0.22343796,\n",
       "         0.11775812,  0.16309094]], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mini_x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "503.984px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "41a4b0350e1457ef5178032865923a48b9ea5a7e4b0371894b5e037efd84882a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
